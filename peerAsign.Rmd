---
title: "Practical Machine Learning Project"
author: "Kamal Bakari"
date: "October 19, 2015"
output: html_document
---

##Executive Summary
This document presents the results of the Practical Machine Learning Project.

Since we have dataset with to many columns and we need make a class prediction, we decide to implement a random forests model, that is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. Before applying the dataset to our prediction model, we will remove all the columns that having less than 60% of data filled, instead of trying to fill them with some center measure. Our model accuracy over validation dataset is equal to 99.9235%. This model promoted a excellent prediction results with our testing dataset and generated the 20th files answers to submit for the Assignments.

##Requirements 
This assignment instructions request us to:
1. predict the manner in which they did the exercise. This is the `classe` variable in the training set. All other variables can be use as predictor. 
2. Show how we built our model, how we used cross validation, what the expected out of sample error is, and why we made the choices we did. 
3. This prediction model also to predict 20 different test cases from the test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

###Preparing the Environment
Here we are setting the working diretory and loading all the required libraries. We are also setting a seed for the experiment for reproductibility.


```{r}
setwd("/Users/hp/Documents/R/pracmachlearn_project")

library(knitr)
library(ElemStatLearn)
library(caret)
library(rpart)
library(randomForest)
set.seed(357)
list.files()
```

###Loading required data(testing and training) and Proprocessing the data to remove NA's.

```{r, echo=TRUE, results='hold'}
trainset <- read.csv("pml-training.csv", header=TRUE, sep=",", na.strings=c("NA",""))
testset <- read.csv("pml-testing.csv", header=TRUE, sep=",", na.strings=c("NA",""))

trainset <- trainset[,-1] #unwanted id column 
inTrain = createDataPartition(trainset$classe, p=0.60, list=FALSE)
training = trainset[inTrain,]
validating = trainset[-inTrain,]

#keep only the required columns
Keep <- c((colSums(!is.na(training[,-ncol(training)])) >= 0.6*nrow(training)))
training   <-  training[,Keep]
validating <- validating[,Keep]
```

###Build a Random Forest Model and show the results. Find the model importances and display the confusion matrix. Also find the accuracy of the prediction.

```{r, echo=TRUE, results='hold'}
model <- randomForest(classe~.,data=training)
print(model)

importance(model)
confusionMatrix(predict(model,newdata=validating[,-ncol(validating)]),validating$classe)
accurancy<-c(as.numeric(predict(model,newdata=validating[,-ncol(validating)])==validating$classe))
accurancy<-sum(accurancy)*100/nrow(validating)

plot(model, log ="y", lwd = 2, main = "Random forest accuracy")

```

##Testing
After the model is build now test the model with the testing set.
Display the results from testing.

```{r, echo=TRUE, results='hold'}
testset <- testset[,-1] # Remove the first column that represents a ID Row
testset <- testset[ , Keep] # Keep the same columns of testing dataset
testset <- testset[,-ncol(testset)] # Remove the problem ID

# Coerce testing dataset to same class and strucuture of training dataset 
testing <- rbind(training[100, -59] , testset) 
# Apply the ID Row to row.names and 100 for dummy row from testing dataset 
row.names(testing) <- c(100, 1:20)

predictions <- predict(model,newdata=testing[-1,])
print(predictions)

```
*** Out of Sample error
We find the Out of sample rate in the following way. 
It is seen that the Out of sample error estimation: 0.08%

```{r, echo=TRUE, results='hold'}
#How big is the validation set?
dim(validating)

predictions <- predict(model,newdata=testing[-1,])
print(predictions)
length(predictions)

# true accuracy of the predicted model
p2 <- predict(model, validating)
outOfSampleError.accuracy <- sum(p2 == validating$classe)/length(p2)
outOfSampleError.accuracy

# out of sample error and percentage of out of sample error
outOfSampleError <- 1 - outOfSampleError.accuracy
outOfSampleError

e <- outOfSampleError * 100
paste0("Out of sample error estimation: ", round(e, digits = 2), "%")


```

#Writting Predictions to files
Write the results into seperate files.

```{r, echo=TRUE, results='hold'}
write_files <- function(x) {
  n <- length(x)
  for (i in 1:n) {
    filename <- paste0("problem_id", i, ".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE,col.names=FALSE)
  }
}

write_files(predictions)

```